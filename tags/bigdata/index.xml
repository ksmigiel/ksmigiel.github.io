<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Bigdata on </title>
    <link>http://ksmigiel.com/tags/bigdata/</link>
    <description>Recent content in Bigdata on </description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Tue, 24 Jan 2017 18:59:09 +0100</lastBuildDate>
    <atom:link href="http://ksmigiel.com/tags/bigdata/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Cuckoo Filters</title>
      <link>http://ksmigiel.com/2017/01/cuckoo-filters</link>
      <pubDate>Tue, 24 Jan 2017 18:59:09 +0100</pubDate>
      
      <guid>http://ksmigiel.com/2017/01/cuckoo-filters</guid>
      <description>

&lt;p&gt;&lt;strong&gt;Cuckoo Filter&lt;/strong&gt; to &lt;strong&gt;probabilistyczna struktura danych&lt;/strong&gt; - podobnie jak &lt;a href=&#34;http://ksmigiel.com/2016/06/bloom-filters/&#34;&gt;Bloom Filter&lt;/a&gt;. W poprzednim poście znajdziecie krótki opis czym taka struktura danych się charakteryzuje (fałszywie dodatki wynik zapytania: &amp;ldquo;czy element zawiera się w danym zbiorze&amp;rdquo;).&lt;/p&gt;

&lt;h2 id=&#34;o-czym-mowa:9fc3faad544296f306615c66d2b26be0&#34;&gt;O czym mowa?&lt;/h2&gt;

&lt;p&gt;Filtr kukułczy jest relatywnie młodą strukturą danych opisaną w 2014 roku przez &lt;a href=&#34;https://www.cs.cmu.edu/~dga/papers/cuckoo-conext2014.pdf&#34;&gt;Fan, Andersen, Kaminsky i Mitzenmacher&lt;/a&gt;. Poszerza ona wspomniane filtry bloom&amp;rsquo;a o &lt;strong&gt;usuwanie&lt;/strong&gt; i &lt;strong&gt;zliczanie&lt;/strong&gt; dodanych elementów, utrzymując przy tym porównywalną złożoność obliczeniową. Minimalizuje ona zasoby przechowując jedynie &lt;strong&gt;odcisk&lt;/strong&gt; &lt;em&gt;(ang. fingerprint)&lt;/em&gt; wartości elementu w zbiorze. W rzeczywistości jest to pewnego rodzaju tablica haszująca, która problem kolizji rozwiązuje za pomocą &lt;a href=&#34;https://en.wikipedia.org/wiki/Cuckoo_hashing&#34;&gt;&lt;strong&gt;haszowania kukułczego&lt;/strong&gt;&lt;/a&gt; &lt;em&gt;(ang. cuckoo hashing)&lt;/em&gt;.&lt;/p&gt;

&lt;h2 id=&#34;cuckoo-hashing:9fc3faad544296f306615c66d2b26be0&#34;&gt;Cuckoo hashing&lt;/h2&gt;

&lt;p&gt;Jak nietrudno się domyślić, nazwa filtra i haszowania wzięła się od &lt;strong&gt;kukułek&lt;/strong&gt;. Kukułki znane są ze składania jaj w obcych gniazdach. Gdy mała kukułka wykluje się, eliminuje ona przybrane rodzeństwo usuwając je z gniazda. Nazywamy to &lt;a href=&#34;https://pl.wikipedia.org/wiki/Paso%C5%BCyty_l%C4%99gowe&#34;&gt;pasożytnictwem lęgowym&lt;/a&gt;. Na podobnej zasadzie opiera się właśnie działanie haszowania/filtra kukułczego.&lt;/p&gt;

&lt;p&gt;W przypadku haszowania każdy klucz jest haszowany przez &lt;strong&gt;dwie różne&lt;/strong&gt; funkcje haszujące, gdzie każdej przyporządkowujemy tablicę, do której będzie można dodawać elementy. Jeżeli miejsce pod zadanym indeksem w pierwszej tablicy jest puste, możemy tam umieścić zadany element. Jeżeli miejsce to jest zajęte, próbujemy dodać element do tablicy drugiej (haszując drugą funkcją). Gdy to miejsce również jest zajęte, &amp;ldquo;eksmitujemy&amp;rdquo; element tam obecny i umieszczamy w to miejsce naszą wartość.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://adriancolyer.files.wordpress.com/2016/10/cuckoo-1.png?w=600&#34; alt=&#34;cuckoo&#34; /&gt;&lt;/p&gt;

&lt;h6 id=&#34;grafika-z-https-adriancolyer-files-wordpress-com:9fc3faad544296f306615c66d2b26be0&#34;&gt;&lt;em&gt;(grafika z &lt;a href=&#34;https://adriancolyer.files.wordpress.com&#34;&gt;https://adriancolyer.files.wordpress.com&lt;/a&gt;)&lt;/em&gt;&lt;/h6&gt;

&lt;p&gt;Na chwilę obecną mamy jeden nigdzie nieprzypisany element (ten usunięty z drugiej tablicy w poprzedniej iteracji). Ponieważ istnieją dwie funkcje/tablice, to użyjemy tego faktu do wyliczenia nowego miejsca w tablicy przeciwnej (pierwszej) i tam spróbujemy umieścić element. Gdy nawet w tym przypadku napotkamy na kolizję, dokonamy eksmisji kolejnego elementu i powtórzymy ten proces aż do momentu znalezienia miejsca w którejś z tablic.&lt;/p&gt;

&lt;h2 id=&#34;cuckoo-filter-zasada-działania:9fc3faad544296f306615c66d2b26be0&#34;&gt;Cuckoo filter - zasada działania&lt;/h2&gt;

&lt;p&gt;Jak już zostało wspomniane, działanie samego filtra opiera się na powyższej strategii i przedstawia się następująco:
Filtr przechowuje &amp;ldquo;odcisk&amp;rdquo; każdego dodanego elementu w jednym z wielu &amp;ldquo;gniazd&amp;rdquo; &lt;em&gt;(ang. bucket)&lt;/em&gt; (odcisk jest ciągiem znaków pochodzącym z wartości hasza). Każde gniazdo posiada swoją wielkość &lt;em&gt;(ang. capacity)&lt;/em&gt;, czyli ile fingerprintów jest w stanie pomieścić. Przyjęło się identyfikować filtr poprzez rozmiar odcisku oraz wielkość gniazda właśnie. Np. filtr (2,4) przechowuje odciski o długości 2 znaków w 4-elementowych koszach (gniazdach).
&lt;img src=&#34;http://ksmigiel.com/images/cuckoo/filtr.png&#34; alt=&#34;filtr&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Ze wszystkich operacji jakie oferuje filtr (dodawanie, usuwanie, sprawdzanie), dodawanie jest najbardziej skomplikowane.
Aby dodać element potrzebujemy dwóch indeksów gniazd na podstawie hasza elementu i jego odcisku.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-csharp&#34;&gt;var hashedElement = Hash(element);
var index1 = GetIndexFromHash(hashedElement);
var fingerprint = GetFingerprint(hashedElement);

// index2 -&amp;gt; index1 XOR index uzyskany z hasza odcisku
var hashedFingerprint = Hash(fingerprint);
var index2 = index1 ^ GetIndexFromHash(hashedFingerprint);
var index2 = index2 % filterCapacity;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Próbujemy dodać element do gniazda spod indeksu 1-szego, a gdy ten jest pełny, to do 2-giego&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-csharp&#34;&gt;if (filter.buckets[index1].Insert(fingerprint))
{
    filterSize++;
    return;
}

if (filter.buckets[index2].Insert(fingerprint))
{
    filterSize++;
    return;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I podobnie jak w opisie wyżej dotyczącym haszowania kukułczego, będziemy próbować przetasowywać elementy w tablicach aż do skutku (ustalonej z góry liczby powtórzeń) lub całkowitego wyczerpania miejsca, gdy próba dodania pod dwa indeksy się nie udała.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://ksmigiel.com/images/cuckoo/insert.png&#34; alt=&#34;insert&#34; /&gt;&lt;/p&gt;

&lt;h6 id=&#34;b2-próbujemy-dodać-do-bucket-1-ale-jest-tam-element-c1-który-zostaje-wyeksmitowany-do-bucket-0-ale-jest-też-dla-niego-wolne-miejsce-w-alternatywnym-bucket-2:9fc3faad544296f306615c66d2b26be0&#34;&gt;&lt;strong&gt;b2&lt;/strong&gt; próbujemy dodać do &lt;strong&gt;bucket[1]&lt;/strong&gt;, ale jest tam element &lt;strong&gt;c1&lt;/strong&gt;, który zostaje wyeksmitowany do &lt;strong&gt;bucket[0]&lt;/strong&gt;, ale jest też dla niego wolne miejsce w alternatywnym &lt;strong&gt;bucket[2]&lt;/strong&gt;.&lt;/h6&gt;

&lt;h2 id=&#34;jeżeli-nie-widać-różnicy-to-po-co-przepłacać:9fc3faad544296f306615c66d2b26be0&#34;&gt;Jeżeli nie widać różnicy, to po co przepłacać?&lt;/h2&gt;

&lt;p&gt;Faworyzowałbym użycie Cuckoo Filter, dopóki aplikacja nie dodaje nowych danych (w dużej ilości) do filtra w krótkich odstępach czasu. Ze względu na rekursywną naturę algorytmu dodającego nowe elementy, która objawia się w momencie przepełnienia filtra, wydajność w porównaniu do Bloom Filtra wypada kiepsko. Autor publikacji dobrze to podsumował:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;[&amp;hellip;] for reasonably large sized sets, for the same false positive rate as a corresponding Bloom filter, cuckoo filters use less space than Bloom filters, are faster on lookups (but slower on insertions/to construct), and amazingly also allow deletions of keys (which Bloom filters cannot do)&lt;/p&gt;

&lt;h6 id=&#34;michael-mitzenmacher-2014-5:9fc3faad544296f306615c66d2b26be0&#34;&gt;&lt;a href=&#34;http://mybiasedcoin.blogspot.com/2014/10/cuckoo-filters.html&#34;&gt;Michael Mitzenmacher (2014)&lt;/a&gt;&lt;/h6&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;em&gt;PS. Pracuję nad implementacją filtra pod .NET Core, więc sprawdzajcie mojego githuba :)&lt;/em&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;http://ksmigiel.com/2016/06/bloom-filters/&#34;&gt;http://ksmigiel.com/2016/06/bloom-filters/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.cs.cmu.edu/~dga/papers/cuckoo-conext2014.pdf&#34;&gt;https://www.cs.cmu.edu/~dga/papers/cuckoo-conext2014.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Cuckoo_hashing&#34;&gt;https://en.wikipedia.org/wiki/Cuckoo_hashing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://pl.wikipedia.org/wiki/Paso%C5%BCyty_l%C4%99gowe&#34;&gt;https://pl.wikipedia.org/wiki/Paso%C5%BCyty_l%C4%99gowe&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://mybiasedcoin.blogspot.com/2014/10/cuckoo-filters.html&#34;&gt;http://mybiasedcoin.blogspot.com/2014/10/cuckoo-filters.html&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Filtry Blooma</title>
      <link>http://ksmigiel.com/2016/06/bloom-filters</link>
      <pubDate>Sat, 11 Jun 2016 11:59:09 +0100</pubDate>
      
      <guid>http://ksmigiel.com/2016/06/bloom-filters</guid>
      <description>

&lt;p&gt;Dzisiaj będzie trochę bardziej nisko poziomowo (nie mylić z niższym poziomem posta). Postaram się w przystepny sposób przybliżyć wam czym jest Filtr Blooma i jak można go zaimplementować w JavaScripcie.&lt;/p&gt;

&lt;h2 id=&#34;filtra-teorie:15fbb8d9ed16407d5b96391271a178aa&#34;&gt;Filtra teorie&lt;/h2&gt;

&lt;p&gt;Filtr Blooma to struktura danych pozwalająca w sposób szybki i pamięciowo optymalny odpowiedzieć na pytanie, czy dany element znajduje się w zbiorze. Niestety, ponieważ nie ma nic za darmo, za wydajność musimy zapłacić, a zapłatą będzie błąd w jaki struktura może nas wprowadzić. Nie użyłem &lt;strong&gt;probabilistyczna&lt;/strong&gt; bez powodu: filtr może stwierdzić jedynie, że elementu &lt;strong&gt;na pewno nie ma&lt;/strong&gt;, lub &lt;strong&gt;może jest&lt;/strong&gt; w zbiorze. Założenie to prowadzi do wniosków tzw. &lt;strong&gt;false-positive&lt;/strong&gt;, czyli że dany element nie istnieje w zadanym zbiorze, a jednak otrzymamy informację o jego prawdopodobnym istnieniu.&lt;/p&gt;

&lt;h3 id=&#34;budowa-i-parametry-filtra:15fbb8d9ed16407d5b96391271a178aa&#34;&gt;Budowa i parametry filtra&lt;/h3&gt;

&lt;p&gt;Filtr można opisać mniej więcej tak:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Filtr Blooma to &lt;strong&gt;tablica&lt;/strong&gt; (wektor) &lt;em&gt;m&lt;/em&gt;-bitów, który ma &amp;ldquo;przewidywać&amp;rdquo; istnienie &lt;em&gt;n&lt;/em&gt; elementów. Elementy te zostały zakodowane &lt;em&gt;k&lt;/em&gt; funkcjami haszującymi.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Dobór tych parametrów wpływa na prawdopodobieństwo wystąpienia błędu (które chcemy minimalizować). &lt;a href=&#34;https://en.wikipedia.org/wiki/Bloom_filter&#34;&gt;Wikipedia&lt;/a&gt; bardzo ładnie przedstawia to od matematycznej strony. Najważniejsze są dwa wzory:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://wikimedia.org/api/rest_v1/media/math/render/svg/76ffa4de74f3857f41900292d0fc315170cec674&#34; alt=&#34;k&#34; /&gt;
&lt;img src=&#34;https://wikimedia.org/api/rest_v1/media/math/render/svg/25b30f6928fac097a6e25aa7b7870a7722b7aea0&#34; alt=&#34;m&#34; /&gt;&lt;/p&gt;

&lt;p&gt;My do teorii aż tak wagi przywiązywać nie będziemy, ale trzeba pamiętać, że przy produkcyjnym użyciu takiej struktury bez wstępnej analizy się nie obejdzie.&lt;/p&gt;

&lt;h3 id=&#34;działanie-filtra:15fbb8d9ed16407d5b96391271a178aa&#34;&gt;Działanie filtra&lt;/h3&gt;

&lt;p&gt;Całość sprowadza się do testu, czy dany bit (lub &lt;em&gt;k&lt;/em&gt; bitów w przypadku &lt;em&gt;k &amp;gt; 1&lt;/em&gt;) dla zadanej wartości (np. &lt;em&gt;x&lt;/em&gt;) jest zapalonych w tablicy. Jeśli choć jeden nie jest - mamy pewność, że element jest nieobecny.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://upload.wikimedia.org/wikipedia/commons/thumb/a/ac/Bloom_filter.svg/649px-Bloom_filter.svg.png&#34; alt=&#34;bloom&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;haszowanie:15fbb8d9ed16407d5b96391271a178aa&#34;&gt;Haszowanie&lt;/h3&gt;

&lt;p&gt;Orłem z kryptografii nie jestem, ale wyczytałem, że użyte funkcje haszujące powinny być przede wszystkim &lt;strong&gt;niezależne&lt;/strong&gt; i o &lt;strong&gt;jednostajnym rozkładzie prawdopodobieństwa&lt;/strong&gt;, np. takie jak: &lt;a href=&#34;https://sites.google.com/site/murmurhash/&#34;&gt;murmur&lt;/a&gt; lub &lt;a href=&#34;https://en.wikipedia.org/wiki/Fowler–Noll–Vo_hash_function&#34;&gt;fnv&lt;/a&gt;. Zalecam dalszą lekturę w tym zakresie dla zainteresowanych szczegółami.&lt;/p&gt;

&lt;h4 id=&#34;podwójne-haszowanie:15fbb8d9ed16407d5b96391271a178aa&#34;&gt;Podwójne haszowanie&lt;/h4&gt;

&lt;p&gt;Jak i jakich funkcji używać do haszowania? Aby zapewnić prawdopodobieństwo błędu na podobnym poziomie, możemy skorzystać z &lt;a href=&#34;http://citeseer.ist.psu.edu/viewdoc/download;jsessionid=4060353E67A356EF9528D2C57C064F5A?doi=10.1.1.152.579&amp;amp;rep=rep1&amp;amp;type=pdf&#34;&gt;&lt;strong&gt;podwójnego haszowania&lt;/strong&gt;&lt;/a&gt;:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://en.wikipedia.org/api/rest_v1/media/math/render/svg/64111088b311219da1c0bc477bf1d97ee0c42b69&#34; alt=&#34;double hash&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Będziemy potrzebować wtedy jedynie dwóch funkcji haszujących &lt;em&gt;h&lt;sub&gt;1&lt;/sub&gt;&lt;/em&gt; i  &lt;em&gt;h&lt;sub&gt;2&lt;/sub&gt;&lt;/em&gt;, &lt;em&gt;i&lt;/em&gt; - numer funkcji (nasze k), &lt;em&gt;T&lt;/em&gt; (nasze m). (&lt;em&gt;Przepraszam za zamieszanie z parametrami, ale wzory są żywcem z Wikipedii&lt;/em&gt;).&lt;/p&gt;

&lt;p&gt;Myślę, że powoli możemy przejść do kodu i próby zaimplementowania tego wszystkiego, co wyżej opisałem.&lt;/p&gt;

&lt;h2 id=&#34;implementacja:15fbb8d9ed16407d5b96391271a178aa&#34;&gt;Implementacja&lt;/h2&gt;

&lt;p&gt;Dla celów edukacyjnych przyjmijmy następujące parametry:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;k = 1    Jedna funkcja haszująca fnv-1
m = 16   16-bitowa tablica
n = 2    Dwa elementy
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;fnv-1:15fbb8d9ed16407d5b96391271a178aa&#34;&gt;fnv-1&lt;/h3&gt;

&lt;p&gt;Wybrałem tę, bo jest relatywnie prosta w implementacji:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-js&#34;&gt;// Operacje na bitach w JS są 32-bitowe
// https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Operators/Bitwise_Operators
//
// Stałe dla 32-bitów
// http://www.isthe.com/chongo/tech/comp/fnv/index.html#FNV-param
const FNV_OFFSET = 0x811c9dc5;
const FNV_PRIME = 0x01000193;

function fnv1(string) {
  let bytes = stringToBytes(string);
  let hash = FNV_OFFSET;

  for (let byte of bytes) {
    hash = hash * FNV_PRIME;
    hash = hash ^ byte;
  }

  return Math.abs(hash);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;stringtobytes:15fbb8d9ed16407d5b96391271a178aa&#34;&gt;stringToBytes&lt;/h3&gt;

&lt;p&gt;Powyższa funkcja potrzebuje tablicy bajtów z ciągu znaków. JS nie posiada takiej ładnej funkcji jak chociażby C# &lt;code&gt;byte[] bytes = encoding.GetBytes(AnyString)&lt;/code&gt;, dlatego musimy napisać coś podobnego. Dla ułatwienia przyjmijmy, że ciąg znaków kodowany jest w ASCII (basic English).&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-js&#34;&gt;function stringToBytes(string) {
  let bytes = [];

  for (let char of string) {
    bytes.push(char.charCodeAt(0));
  }
  
  return bytes;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;bloomfilter:15fbb8d9ed16407d5b96391271a178aa&#34;&gt;BloomFilter&lt;/h3&gt;

&lt;p&gt;Jak zauważyliście używam składni ES6, o co by iść z duchem czasu. Filtr będzie przyjmował w konstruktorze liczbę bitów oraz funkcje haszujące, których chcemy użyć.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-js&#34;&gt;class BloomFilter {
  constructor (m, k) {
    this.bits = m;
    this.hashFunctions = k;
    this.filter = new Int32Array(m);
  }

  add (value) {
    for (let hash of this.hashFunctions) {
      let hashIndex = hash(value) % this.bits;
      this.filter[hashIndex] = 1;
    }
  }

  check (value) {
    for (let hash of this.hashFunctions) {
      let hashIndex = hash(value) % this.bits;
      if (this.filter[hashIndex] == 0)
        return false;
    }
    return true;
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;testujemy:15fbb8d9ed16407d5b96391271a178aa&#34;&gt;Testujemy!&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-js&#34;&gt;var bloomFilter = new BloomFilter(16, [ fnv1 ]);

bloomFilter.add(&#39;testujemy!&#39;);
bloomFilter.add(&#39;filtr&#39;);

bloomFilter.check(&#39;testujemy!&#39;);
// true
bloomFilter.check(&#39;filtr&#39;);
// true
bloomFilter.check(&#39;nie ma&#39;)
// false
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Prawdopodobieństwo błędu dla przyjętych parametrów wynosi:
&lt;img src=&#34;https://www4c.wolframalpha.com/Calculate/MSP/MSP772420gih62f6ech2f05000010ahiefh16dba6cd?MSPStoreType=image/gif&amp;amp;s=59&#34; alt=&#34;bloom&#34; /&gt;
&lt;em&gt;p ~= 0.12&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;ale przy 10-ciu elementach wzrasta do &lt;em&gt;p ~= 0.46&lt;/em&gt;, dlatego dobór odpowiednich parametrów jest bardzo ważny.&lt;/p&gt;

&lt;h2 id=&#34;produkcyjne-użycia:15fbb8d9ed16407d5b96391271a178aa&#34;&gt;Produkcyjne użycia&lt;/h2&gt;

&lt;p&gt;Z Filtrów Blooma korzystają m.in:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://cassandra.apache.org/&#34;&gt;Cassandra&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://hadoop.apache.org/&#34;&gt;Hadoop&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://cloud.google.com/bigtable/&#34;&gt;Google BigTable&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.akamai.com/&#34;&gt;Akamai&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Bloom_filter&#34;&gt;https://en.wikipedia.org/wiki/Bloom_filter&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://sites.google.com/site/murmurhash/&#34;&gt;https://sites.google.com/site/murmurhash/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Fowler–Noll–Vo_hash_function&#34;&gt;https://en.wikipedia.org/wiki/Fowler–Noll–Vo_hash_function&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.isthe.com/chongo/tech/comp/fnv/index.html#FNV-param&#34;&gt;http://www.isthe.com/chongo/tech/comp/fnv/index.html#FNV-param&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://citeseer.ist.psu.edu/viewdoc/download;jsessionid=4060353E67A356EF9528D2C57C064F5A?doi=10.1.1.152.579&amp;amp;rep=rep1&amp;amp;type=pdf&#34;&gt;http://citeseer.ist.psu.edu/viewdoc/download;jsessionid=4060353E67A356EF9528D2C57C064F5A?doi=10.1.1.152.579&amp;amp;rep=rep1&amp;amp;type=pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>U-SQL i Azure Data Lake</title>
      <link>http://ksmigiel.com/2015/12/u-sql</link>
      <pubDate>Sun, 20 Dec 2015 20:26:38 +0100</pubDate>
      
      <guid>http://ksmigiel.com/2015/12/u-sql</guid>
      <description>

&lt;p&gt;Nie skłamię twierdząc, że &lt;strong&gt;Big Data&lt;/strong&gt; jest obok &lt;strong&gt;IoT&lt;/strong&gt;, &lt;strong&gt;machine learningu&lt;/strong&gt; czy &lt;strong&gt;drukowania 3D&lt;/strong&gt; w top 5 jeśli chodzi o modne pojęcia i zagadnienia wyznaczające trendy w świecie IT, tworzące nowe gałęzie w tej dziedzinie. Ponieważ Microsoft w ostatnim czasie realizuje politykę bycia &amp;ldquo;na topie&amp;rdquo; (publiczne repozytoria na GitHub&amp;rsquo;ie, .NET na Linuxie - DNX i Kestrel etc.) nie mogło ich też zabraknąć w tak gorącym temacie jakim jest obecnie Big Data. Efektem popularyzacji Hadoopa i jego przyległości było w tym wypadku otworzenie platformy &lt;a href=&#34;https://azure.microsoft.com/pl-pl/services/hdinsight/&#34;&gt;HDInsight&lt;/a&gt; na chmurze Azure. Za pomocą rozbudowanego interfejsu webowego z łatwością możemy tworzyć klastry Hadoop&amp;rsquo;a, a z poziomu Visual Studio używając SDK projektować &lt;a href=&#34;https://azure.microsoft.com/pl-pl/documentation/articles/hdinsight-storm-develop-csharp-visual-studio-topology/&#34;&gt;topologie Storm&amp;rsquo;a&lt;/a&gt; do przetwarzania strumieniowego i wdrażać je prosto do Azure. Dokumentacja zawiera naprawdę sporo informacji na różnym poziomie zaawansowania, a Azure oferuje 30 dni triala, więc nic tylko zakładać konto i eksperymentować.&lt;/p&gt;

&lt;h2 id=&#34;azure-data-lake:0f75fabf8504a87dff62df1f8eafb8e8&#34;&gt;Azure Data Lake&lt;/h2&gt;

&lt;p&gt;Opisany wyżej poziom abstrakcji okazał się niewystarczający. O ile programiści z konfiguracją klastrów i pisaniem zadań MapReduce poradzą sobie bez problemu, to analitycy danych pracujący na co dzień z SQL&amp;rsquo;em i Excelem już nie koniecznie. Chcąc ułatwić osobom &amp;ldquo;mniej technicznym&amp;rdquo; dostęp do technologii, pod koniec września tego roku Microsoft zapowiedział uruchomienie nowego serwisu: &lt;a href=&#34;http://blogs.technet.com/b/dataplatforminsider/archive/2015/09/28/microsoft-expands-azure-data-lake-to-unleash-big-data-productivity.aspx&#34;&gt;&lt;strong&gt;Azure Data Lake&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;div style=&#34;text-align: center&#34;&gt;
  &lt;img src=&#34;http://blogs.technet.com/cfs-file.ashx/__key/communityserver-blogs-components-weblogfiles/00-00-00-60-54/928Pic1.png&#34; /&gt;
&lt;/div&gt;

&lt;p&gt;Jest to serwis, który osiągnięcie konkretnych celów biznesowych stawia nad konfigurację rozproszonej architektury. Pozwala skupić się na logice aplikacji, zamiast na skomplikowanym systemie zależności potrzebnym do jej poprawnego działania. Microsoft inwestując w tego typu technologie Big Data / analizy danych pragranie ułatwić pracę z danymi każdego typu, wielkości i szybkości (czyli tzw. 3xV: &lt;a href=&#34;http://blog.sqlauthority.com/2013/10/02/big-data-what-is-big-data-3-vs-of-big-data-volume-velocity-and-variety-day-2-of-21/&#34;&gt;Volume, Velocity, Variety&lt;/a&gt;) używając do tego dowolnych narzędzi, języków czy frameworków.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;Our goal is to make big data technology simpler and more accessible to the greatest number of people possible. This includes developers, data scientists, analysts, application developers, and also businesspeople and mainstream IT managers.&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Każdy kto próbował samodzielnie uruchomić Hadoopa wraz z całą infrastrukturą wie jak bardzo jest to czasochłonne. Rozwiązanie &amp;ldquo;code-based&amp;rdquo; daje ogromne możliwości, wymaga jednak sporej inwestycji czasu, aby je opanować. Programista często musi zadbać o poprawną synchronizację i współbieżność samodzielnie. Natomiast SQL i języki SQL-podobne, takie jak &lt;a href=&#34;https://hive.apache.org/&#34;&gt;Hive&lt;/a&gt; są relatywnie łatwe, ale brak w nich właśnie tej elastyczności jaką cechuje poprzednie rozwiązanie, jednakże problemy ze skalowalnością, optymalizacją i współbieżnością nie są już tutaj odpowiedzialnością developera.&lt;/p&gt;

&lt;h2 id=&#34;u-sql:0f75fabf8504a87dff62df1f8eafb8e8&#34;&gt;U-SQL&lt;/h2&gt;

&lt;p&gt;I tak o to Microsoft tworzy całkowicie nowy język do analizy danych &lt;strong&gt;U-SQL&lt;/strong&gt;, będący hybrydą dwóch paradygmatów: deklaratywnego i proceduralnego. Teraz z poziomu kodu przypominającego SQL (w rzeczywistości wzorowany na T-SQL i ANSI SQL) możemy korzystać z dobrodziejstw C#, co pozwala na używanie typów jak i wyrażeń lambda (LINQ) m.in w zapytaniu &lt;em&gt;SELECT&lt;/em&gt;. Brzmi niesamowicie, nieprawdaż? Co więcej nic nie stoi na przeszkodzie, aby podpiąć pod zapytanie referencję do biblioteki i użyć zewnętrznego kodu. Daje to przeogromne możliwości w budowaniu rozwiązań analitycznych zarówno dla programistów jak i analityków.
Ponieważ specyfikacja języka nie jest jeszcze w pełni gotowa, a Azure Data Lake jest (jak sam to określa) w &amp;ldquo;wersji zapoznawczej&amp;rdquo;, przedstawię tylko podstawową składnię i smaczki, co by nabrać apetytu na następne posty :)&lt;/p&gt;

&lt;h3 id=&#34;extractory-i-outputtery:0f75fabf8504a87dff62df1f8eafb8e8&#34;&gt;Extractory i Outputtery&lt;/h3&gt;

&lt;p&gt;Azure Data Lake składa się z dwóch serwisów: ADL &lt;strong&gt;Analytics&lt;/strong&gt; i ADL &lt;strong&gt;Store&lt;/strong&gt;. Ten pierwszy został w skrócie opisany wyżej, a drugi to repozytorium danych, które przechowuje dane w różnej postaci (nawet w czasie rzeczywistym, chociażby z urządzeń IoT). Jest ono kompatybilne z systemem plików &lt;strong&gt;HFDS&lt;/strong&gt; przez co Hadoop i dystrybucje bazujące na nim bez przeszkód mogą uzyskać dostęp do danych w celu przetwarzania i analizy. Komunikacja między nimi odbywa się właśnie za pomocą &lt;strong&gt;Extractorów&lt;/strong&gt; i &lt;strong&gt;Outputterów&lt;/strong&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;@searchlog =
    EXTRACT UserId          int,
            Start           DateTime,
            Region          string,
            Query           string,
            Duration        int?,
            Urls            string,
            ClickedUrls     string
    FROM &amp;quot;/Samples/Data/SearchLog.tsv&amp;quot;
    USING Extractors.Tsv();

OUTPUT @searchlog   
    TO &amp;quot;/output/SearchLog-first-usql.csv&amp;quot;
USING Outputters.Csv();
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;W ten sposób importujemy plik SearchLog.tsv (tab separated value) do pamięci i od tego momentu korzystamy ze zmiennej jak ze zwykłej, tymczasowej tabeli. Po zakończeniu analiz eksportujemy plik z powrotem do ADL Store, tym razem w formacie .csv (comma separated value). &lt;code&gt;Tsv()&lt;/code&gt; i &lt;code&gt;Csv()&lt;/code&gt; są w standardzie U-SQL, a przy pomocy SDK możemy napisać klasy do obsługi innych typów danych. Na &lt;a href=&#34;https://github.com/MicrosoftBigData/usql/&#34;&gt;GitHub&amp;rsquo;ie&lt;/a&gt; U-SQL można znaleźć przykładową implementację dla typów &lt;strong&gt;XML&lt;/strong&gt; i &lt;strong&gt;JSON&lt;/strong&gt;.&lt;/p&gt;

&lt;h3 id=&#34;wyrażenia-lambda-linq-i-typy:0f75fabf8504a87dff62df1f8eafb8e8&#34;&gt;Wyrażenia lambda, LINQ i typy&lt;/h3&gt;

&lt;p&gt;Ponieważ typy w U-SQL&amp;rsquo;u są typami z C#, możemy ich używać dokładnie tak samo - wszystkie metody na wyciągnięcie ręki! Dzieje się tak, ponieważ standardowy skrypt U-SQL ma referencję do &lt;code&gt;System&lt;/code&gt; i &lt;code&gt;System.Linq&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;--@tweets =
--    EXTRACT
--    ...
    
@refs = SELECT new SQL.ARRAY&amp;lt;string&amp;gt;(
            tweet.Split(&#39; &#39;)
                 .Where(x =&amp;gt; x.StartsWith(&amp;quot;@&amp;quot;))) AS refs
        FROM @tweets;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Kod wygląda jakby ktoś do SQL&amp;rsquo;a wkleił C#&amp;lsquo;a, a co najważniejsze: działa i to własnie jest U-SQL! :). &lt;code&gt;SQL.ARRAY&amp;lt;T&amp;gt;&lt;/code&gt; jest typem wbudowanym (w rzeczywistości typem C#) zachowującym się jak tabela SQL (możemy &lt;em&gt;@refs&lt;/em&gt; używać w innych zapytaniach, łączyć z tabelami itp.), a ponieważ &lt;code&gt;Where()&lt;/code&gt; zwraca &lt;code&gt;IEnumerable&amp;lt;T&amp;gt;&lt;/code&gt;, w kolejnych zapytaniach nadal możemy filtrować kolekcję za pomocą predykatów.&lt;/p&gt;

&lt;p&gt;W pewwym momencie będziemy musieli nasze typy-hybrydy zmaterializować (coś na kształt &lt;code&gt;ToList()&lt;/code&gt;) i użyjemy do tego &lt;code&gt;CROSS APPLY EXPLODE&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;@t = SELECT Refs.r.Substring(1) AS r,
            &amp;quot;mentioned&amp;quot; AS category
     FROM @refs CROSS APPLY EXPLODE(refs) AS Refs(r);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Tak przetworzone dane bez problemu zapisujemy w wybranym formacie do ADL Store (patrz wyżej).&lt;/p&gt;

&lt;p&gt;Więcej przykładów znajdziecie na podanym GitHubie Microsoftu (linki na samym dole). Przykłady użyć SDK i Azure Data Lake Tools for Visual Studio opiszę w następnym poście.&lt;/p&gt;

&lt;h2 id=&#34;czyli-dla-każdego:0f75fabf8504a87dff62df1f8eafb8e8&#34;&gt;Czyli dla każdego?&lt;/h2&gt;

&lt;p&gt;Chyba tak. Teraz przetwarzanie danych w chmurze stało się jeszcze łatwiejsze. Niski próg wejścia i znajoma składnia z pewnościa przyciągnie do platformy wielu użytkowników skoncentrowanych na osiągnięciu celu biznesowego. Tego typu podejście nie jest szczepionką na wszystko i z pewnością bardziej &amp;ldquo;customowe&amp;rdquo; rozwiązania nie stracą na znaczeniu.&lt;/p&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://azure.microsoft.com/pl-pl/services/hdinsight/&#34;&gt;https://azure.microsoft.com/pl-pl/services/hdinsight/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://azure.microsoft.com/pl-pl/documentation/articles/hdinsight-storm-develop-csharp-visual-studio-topology/&#34;&gt;https://azure.microsoft.com/pl-pl/documentation/articles/hdinsight-storm-develop-csharp-visual-studio-topology/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://blogs.technet.com/b/dataplatforminsider/archive/2015/09/28/microsoft-expands-azure-data-lake-to-unleash-big-data-productivity.aspx&#34;&gt;http://blogs.technet.com/b/dataplatforminsider/archive/2015/09/28/microsoft-expands-azure-data-lake-to-unleash-big-data-productivity.aspx&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://blog.sqlauthority.com/2013/10/02/big-data-what-is-big-data-3-vs-of-big-data-volume-velocity-and-variety-day-2-of-21/&#34;&gt;http://blog.sqlauthority.com/2013/10/02/big-data-what-is-big-data-3-vs-of-big-data-volume-velocity-and-variety-day-2-of-21/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://hive.apache.org/&#34;&gt;https://hive.apache.org/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/MicrosoftBigData/usql/&#34;&gt;https://github.com/MicrosoftBigData/usql/&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
  </channel>
</rss>