<!doctype html>
<html>
<head>
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Hadoop Streaming w F# - Krzysztof Śmigiel | Blog</title>
  <link href="https://fonts.googleapis.com/css?family=Open+Sans:700,600,400" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="/css/main.css">
  <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
  <link href="http://ksmigiel.com//index.xml" rel="alternate" type="application/rss+xml" title="Krzysztof Śmigiel | Blog" />
  <meta name="title" content="http://ksmigiel.com/">
  <link rel="canonical" href="http://ksmigiel.com/">
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.5/styles/github.min.css">
  <script src="/js/highlight.js"></script>
  <meta property="og:title" content="Krzysztof Śmigiel | Blog"/>
  <meta property="og:url" content="http://ksmigiel.com/"/>
  <meta property="og:image" content="http://ksmigiel.com//images/"/>   
  <meta property="og:site_name" content="Krzysztof Śmigiel | Blog">
</head>
<body>
<section class="site-nav">
  <header>
    <nav id="navigation">
      <a href="/" class="home">Blog</a>
      <a href="/tags">Tagi</a>
      <a href="/post">Archiwum</a>
      <a href="/about">O mnie</a>
    </nav>
  </header>
</section>


  
  <article>
    <div class="container">
      <header>
        <div class="meta">
          Autor <address><a rel="author" href="http://ksmigiel.com" title="Krzysztof Śmigiel" target="_blank">Krzysztof Śmigiel</a></address> &mdash;
          <time class="visibility-hidden" pubdate datetime="2015-02-17" title="2015-02-17">17 February 2015</time>
          <ul id="tags">
            
            <li><a href="/tags/hadoop">#hadoop</a> </li>
            
            <li><a href="/tags/mono">#mono</a> </li>
            
            <li><a href="/tags/fsharp">#fsharp</a> </li>
            
          </ul>
        </div>
        <h1 class="title">Hadoop Streaming w F#</h1>
        <h2 class="subtitle">Hortonworks Data Platform i mono</h2>
      </header>

      <section>
        

<p>Klasyczny Hadoop posiada zestaw klas Javowych, dzięki którym możemy napisać swoje pierwsze zadanie <strong>MapReduce</strong>. Jak zaglądniecie do <a href="http://hadoop.apache.org/docs/r1.2.1/mapred_tutorial.html">dokumentacji</a>, to przekonacie się, że nie jest to najwygodniejsze rozwiązanie, ale świetnie nadaje się jako materiał do nauki. Przy zastosowaniach produkcyjnych zdecydowanie lepiej zwrócić się w stronę narzędzi takich jak <a href="https://hive.apache.org/">Hive</a>, <a href="http://pig.apache.org/">Pig</a> czy <a href="https://github.com/twitter/scalding">Scalding</a>. Za pomocą specyficznej dla każdego składni można tworzyć zaawansowane analizy bez potrzeby pisania kodu low-level w Javie, np. <strong>HiveQL</strong> jest językiem zbliżonym do SQL, a <strong>Pig Lating</strong> ciekawym językiem proceduralnym. Oba są kompilowane do zadań MapReduce. W tym poście skupię się na czymś pośrodku, czyli <strong>Hadoop Streaming</strong>.</p>

<h2 id="streaming-api:7428b1f511329ebc1011ee8ee83894f8">Streaming API</h2>

<p>Hadoop Streaming jest częścią dystrybucji Hadoop. Pozwala na tworzenie zadań w dowolnym języku (nawet skryptowym). Warunek jaki trzeba spełnić, to utworzenie dwóch plików wykonywalnych (Mapper i Reducer), które wartości zczytują z <strong>stdin</strong>, a przetworzone odpowiednio dane wypisują do konsoli (<strong>stdout</strong>). Jak tytuł posta wskazuje zaimplementowałem je w F#. Dopiero raczkuję w świecie programowania funkcyjnego, dlatego z chęcią przyjmę komentarze odnośnie poprawności kodu. Za przykładowe zadanie MapReduce posłuży nam standardowy <strong>word count</strong>.</p>

<h4 id="mapper-fs:7428b1f511329ebc1011ee8ee83894f8">Mapper.fs</h4>

<pre><code class="language-fsharp">open System
open System.IO

module Mapper =

  [&lt;EntryPoint&gt;]
  let main argv = 
    let chars =
      [| ' '; '.'; ','; '!'; ';'; '?'; '|'; '-'; '{'; '}'; ':'; '('; ')' |]

    match argv.Length with
    | 1 -&gt; Console.SetIn(new StreamReader(argv.[0]))
    | _ -&gt; ()

    let isWord w =
      let n = ref 0
      not (Int32.TryParse(w, n))

    let output (word:string) =
      Console.WriteLine(&quot;{0}\t{1}&quot;, word.Trim(), 1)

    Seq.initInfinite (fun _ -&gt; Console.ReadLine())
    |&gt; Seq.takeWhile (fun line -&gt; line &lt;&gt; null)
    |&gt; Seq.iter (fun (line : string) -&gt; 
      line.ToLower().Split(chars, StringSplitOptions.RemoveEmptyEntries)
      |&gt; Seq.filter isWord
      |&gt; Seq.iter output )
    0
</code></pre>

<p>Zamiast posługiwać się pętlą przy odczytywaniu streamu z stdin wykorzystałem funkcję <code>Seq.initInfinite()</code>, która wykonuje się aż do spełnienia warunku zdefiniowanego w <code>Seq.takeWhile()</code>. Do konsoli wypisujemy parę &ldquo;<strong>klucz</strong> <strong>wartość</strong>&rdquo; oddzielone znakiem <strong>tabulacji</strong>. Ponieważ interesuje nas zliczanie słów, jako wartość wychodzącą z Mappera podajemy <strong>1</strong>, czyli</p>

<pre><code class="language-bash">word    1
count    1
example    1
</code></pre>

<p>itd. Reducer otrzymuje posortowany już stream takich par i powinien zwracać dane w ten sam sposób, natomiast w miejsce wartości wstawiamy sumę dla danego słowa (klucza).</p>

<h4 id="reducer-fs:7428b1f511329ebc1011ee8ee83894f8">Reducer.fs</h4>

<pre><code class="language-fsharp">open System
open System.IO

module Reducer =

  [&lt;EntryPoint&gt;]
  let main argv = 
    match argv.Length with
    | 1 -&gt; Console.SetIn(new StreamReader(argv.[0]))
    | _ -&gt; ()

    let currentWord = ref String.Empty
    let count = ref 0

    Seq.initInfinite (fun _ -&gt; Console.ReadLine())
    |&gt; Seq.takeWhile (fun line -&gt; line &lt;&gt; null)
    |&gt; Seq.iter (fun line -&gt;
      let splitted = line.Split('\t')
      let word = (splitted.[0])

      match (word) with
      | word when word = !currentWord -&gt;
        incr count
      | _ -&gt;
        if !currentWord &lt;&gt; String.Empty then
          Console.WriteLine(&quot;{0}\t{1}&quot;, !currentWord, !count)
        count := 1
        currentWord := word)
    |&gt; ignore
    Console.WriteLine(&quot;{0}\t{1}&quot;, !currentWord, !count)
</code></pre>

<h2 id="hdp:7428b1f511329ebc1011ee8ee83894f8">HDP</h2>

<p>Hortonworks Data Platform (HDP) to gotowa dystrybucja Hadoopa, która zawiera preinstalowane i skonfigurowane narzędzia takie jak <strong>Hadoop</strong>, <strong>Hive</strong>, <strong>Pig</strong>, <strong>HBase</strong>, <strong>Ambari</strong>, <strong>Cascading</strong>, <strong>Oozie</strong> czy <strong>Zookeeper</strong> <a href="http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.2.0/Getting_Started_v22/media/01-RawContent/Getting%20Started/Full%20View.png">(architektura HDP)</a>. Ogólnie polecam rozwiązania firmy <a href="http://hortonworks.com/">Hortonworks</a> - naprawdę świetnej jakości tutoriale i narzędzia. Chcąc poeksperymentować mamy dwie opcje do wyboru <a href="http://hortonworks.com/hdp/downloads/">(downloads)</a>:</p>

<ul>
<li>instalacja HDP</li>
<li>uruchomienie maszyny wirtualnej (sandbox)</li>
</ul>

<p>Wypóbowałem obydwie, ale na potrzeby tego posta skorzystam z sandboxa. Działa na systemie <strong>CentOS</strong>, także podstawowa wiedza z systemów <strong>Unix</strong> bardzo się przyda.  Instalacja i konfiguracja HDP na Windowsie to temat nadający się na osobnego posta.</p>

<h2 id="mono:7428b1f511329ebc1011ee8ee83894f8">mono</h2>

<p>Musimy sami doinstalować <code>mono</code> i kompilator <code>fsharpc</code>, żeby binarki Hadoopa mogły wykonać z terminala skompilowane pliki .exe.</p>

<pre><code class="language-bash">sudo yum install mono
sudo yum install fsharp
</code></pre>

<p>Teraz wystarczy skompilować nasze pliki:</p>

<pre><code class="language-bash">fsharpc Mapper.fs
fsharpc Reducer.fs
</code></pre>

<p>i możemy zacząc prawdziwą zabawę :)</p>

<h2 id="hello-world-dla-big-data-czyli-word-count:7428b1f511329ebc1011ee8ee83894f8">Hello world dla Big Data, czyli word count</h2>

<p>Nie przez przypadek wybrałem zliczanie słów jako przykład. Jest to swoisty &ldquo;Hello world!&rdquo; w świecie przetwarzania danych. Zliczymy 10 najczęściej występujących słów (dłuższych niż 3 litery, aby pozbyć się &ldquo;się&rdquo; i &ldquo;aby&rdquo;) w powieści <strong>Krzyżacy</strong> Henryka Sienkiewicza. Wszystkie pliki i skrypty znajdziecie na moim <a href="https://github.com/ksmigiel/hadoop-streaming-fharp">githubie</a>.</p>

<p>Uruchomienie joba odbywa się za pomocą komendy:</p>

<pre><code class="language-bash">hadoop jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-streaming.jar
    -files mapper.sh, reducer.sh, Mapper.exe, Reducer.exe
    -input /user/ksmigiel/krzyzacy.txt
    -output /users/ksmigiel/output
    -mapper mapper.sh
    -reducer reducer.sh
</code></pre>

<p><strong>mapper.sh</strong> i <strong>reducer.sh</strong> to skrypty, które wykonują polecenie <code>mono [exe]</code>.
Input i output to ścieżka na HDFS, dlatego trzeba skopiować plik za pomocą <code>hdfs dfs -copyFromLocal &lt;src&gt; &lt;dest&gt;</code>.
Jeśli będziecie chcieli użyć dowolnego pliku tekstowego, pamiętajcie o kodowaniu w UTF-8 i konwersji znaków nowej linii za pomocą <code>dos2unix</code>.</p>

<p>I jeszcze na zakończenie skrypcik <strong>Pig</strong>:</p>

<pre><code class="language-sql">words = load '/user/ksmigiel/out/part-00000' using PigStorage() as (word:chararray, count:int);
words_long = filter words by size(word) &gt; 3;
words_ordered = order words_long by count desc;
top10 = limit words_ordered 10;
dump top10;
</code></pre>

<div>
  <a href="https://plot.ly/~ksmigiel/17/" target="_blank" title="Krzyżacy word count" style="display: block; text-align: center;"><img src="https://plot.ly/~ksmigiel/17.png" alt="Krzyżacy word count" style="max-width: 100%;width: 564px;"  width="564" onerror="this.onerror=null;this.src='https://plot.ly/404.png';" /></a>
  <script data-plotly="ksmigiel:17" src="https://plot.ly/embed.js" async></script>
</div>

<h2 id="podsumowanie:7428b1f511329ebc1011ee8ee83894f8">Podsumowanie</h2>

<p>Stawianie pierwszych kroków z Hadoopem i jego przyległościami wymaga paru wolnych chwil. Jeśli nie macie doświadczenia z linuxowym terminalem, poruszanie się po sandboxie i jego obsługa mogą być kłopotliwe. Co prawda HDP udostępnia przyjemny interfejs webowy, z poziomu którego można uruchamiać zadania MapReduce napisanie w Pig lub HiveQL (jeszcze raz polecam tutoriale Hortonworks), ale chcąc poznać podstawy tej technologii dobrze jest zacząć od &ldquo;niskiego poziomu&rdquo;.</p>

        <div class="social">
  <div>
    <a class="twitter-share-button"
      href="https://twitter.com/share"
      data-via="k_smigiel"
      data-url="http://ksmigiel.com/2015/02/hadoop-streaming">
      Tweet
    </a> 
  </div>
  <div>
    <div class="fb-like" data-width="450" data-layout="button_count" data-action="like" data-show-faces="false" data-send="false"></div>
  </div>
</div>


      </section>

      
      <section>
        <div id="disqus_thread"></div>
<script type="text/javascript">
   
  var disqus_shortname = 'ksmigiel'; 

   
  (function() {
     var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
     dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
     (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
   })();
</script>

      </section>
      

    </div>
  </article>
<footer class="site-footer">
  <div class="container">
    <nav>
      <a href="/">Blog</a> &middot;
      <a href="/tags">Tagi</a> &middot;
      <a href="/post">Archiwum</a> &middot;
      <a href="/about">O mnie</a>
    </nav>

    <nav class="social">
      
      <a href="https://twitter.com/k_smigiel" title="Follow on Twitter" target="_blank"><i class="icon icon-twitter black"></i></a>
      
      
      <a href="https://github.com/ksmigiel" title="Follow on Github" target="_blank"><i class="icon icon-github black"></i></a>
      
          
      <a href="/index.xml" title="RSS Feed">
        <i class="icon icon-rss black"></i>
      </a>
    </nav>
  </div>
</footer>
<script type="text/javascript" src="//ajax.googleapis.com/ajax/libs/jquery/1.7.2/jquery.min.js"></script>
<script type="text/javascript" src="/js/main.js"></script>


<script>window.twttr=(function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],t=window.twttr||{};if(d.getElementById(id))return;js=d.createElement(s);js.id=id;js.src="https://platform.twitter.com/widgets.js";fjs.parentNode.insertBefore(js,fjs);t._e=[];t.ready=function(f){t._e.push(f);};return t;}(document,"script","twitter-wjs"));</script>



<div id="fb-root"></div>
<script>(function(d, s, id) {
var js, fjs = d.getElementsByTagName(s)[0];
if (d.getElementById(id)) return;
js = d.createElement(s); js.id = id;
js.src = "//connect.facebook.net/pl_PL/sdk.js#xfbml=1&version=v2.2&appId=901391149893781";
fjs.parentNode.insertBefore(js, fjs);
}(document, 'script', 'facebook-jssdk'));</script>



  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-58701947-1', 'auto');
  ga('send', 'pageview');

</script>






</body>
</html>

